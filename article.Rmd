---
title: "Funding programmes for early career researchers do/not influence their independence"
author: "David Janků"
date: "`r Sys.Date()`"
output: html_document
bibliography: researcher_independence.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(targets)
library(tidyr)
library(dplyr)
library(cobalt)
library(knitr)
library(psych)
library(kableExtra)
library(ggplot2)
```

# Intro

Researchers’ independence from their PhD advisor is one of the criteria often used for assessing researchers for hiring, tenure and promotion (Van Arensbergen, Van der Weijden, Van den Besselaar, 2014; Moher et al, 2018), constituting one factor of research excellence.
The promise of independence is that researchers will “challenge the existing common views and open up new lines of research” (Van den Besselaar & Sandström, 2019, p.2). 

An independent investigator is then defined by the US National Academies of Science (NAS) as “one who enjoys independence of thought—the freedom to define the problem of interest and to choose or develop the best strategies and approaches to address that problem–especially during the earlier career phase” (Van den Besselaar & Sandström, 2019, p.4). Such a definition does not necessarily put independence in relation to other actors (e.g. one’s PhD advisor), but only in relation to one’s own freedom and resources. However other institutions, such as European Research Council via their Starting Grants scheme (Neufeld, Huber & Wegner, 2013) think of independence in a relational manner, in terms of having one (or a few) publications without the former PhD supervisor being co-author; being PI on a grant; or being lab director or research leader. 

Based on some early empirical studies (Rojko & Lužar, 2022), it seems that researchers in disciplines such as philosophy, psychology; religion, theology; social sciences; the arts, recreation, entertainment, sport; language, linguistics, literature; and geography, biography, history experience significantly higher levels of indepedence on their PhD advisor than researchers in disciplines such as science and knowledge, organization, computer science, information, documentation, librarianship, institutions, publications; mathematics, natural sciences; and applied sciences, medicine, technology. 
There is also some preliminary evidence that higher indepedence modestly correlates with number of citations aquired (Rojko & Lužar, 2022), but others argue that even the research systems with very low indepedence can create a lot of scientific impact, and thus the relationships between independence and impact might not be as clear (Gläser et al., 2021)


In recent decades, there has been a worry that researchers’ indepedence is decreasing, illustrated by increase in age at which US (biomedical) researchers received their first grant (NAS, 2005; Daniels, 2015), as well as direct measures of new reserchers becoming more depedent on their PhD advisors across almost all disciplines in Slovenia (Rojko & Lužar, 2022, table 6 on p.9). Perhaps to tackle this trend, many grant agencies decided to start awarding grants for early career researchers, usually up to 8 years after finishing their PhD. Such programmes have often explicitly stated goals of helping researchers become more indepedent, set up their own research group and open up new lines of research. The examples of such programmes could be Starting Grants by European Research Council (2022), Junior project and Junior Star by Czech Science Foundation (2020), Walter N. Benjamin Programme by DFG (2022), Innovation Research Incentives Scheme by Netherlands Organization of Scientific Research (NWO) (2021)... something from the US would be also good)


Although researcher independence seems to be important feature that is acknowledged by many actors in the research systems and that research funders try to stimulate, there hasn’t been much effort focused on quantifying this phenomenon. Some studies try to measure independence in a simple way, as a ratio of publications the given researcher produced without their supervisor divided by the total number of publications of the given researchers (e.g. Rojko & Lužar, 2022). However, such indicator could be misleading in the world of team science, and also only focuses on collaborations, not tracking whether given researcher become independent in their thinking and research topics. [@vandenbesselaarMeasuringResearcherIndependence2019] therefore proposed more complex indicator for measuring researcher independence on the individual level, based on co-authorship network and topical overlaps. Their measure uses only information about publications’ characteristics (authorship and reference list) regardless of it’s impacts, unlike other alternative measures that focus solely on the impact of the independent work in terms of received citations (Dey, et al., 2021).

Since the explicit goal of many funding instruments that are being used by research funders all over the world is to help early career researchers become independent, we would like to use the newly created indicators to assess the extent, to which such programmes influence researchers independence.    

H1: Researchers, who received funding from programmes aimed at funding early career researchers, will show more independence on their PhD advisor 5 years after receiving the funding than researchers who did not receive this kind of funding.

# Method

## Sample / data

We used the database of all publications produced by researchers employed at Czech research institutions (RIV) combined with a database of all publicly funded research projects and grants in the Czech Republic (CEP), both publicly accessible at https://www.isvavai.cz/, and transformed into a single database with cleaned data, unique person and publication identifiers, and better structure (Hladik, XXX).
This allowed us to identify all researchers who received Junior Grants from the Czech Science Foundation, a funding programme that has an explicit goal to help early career researchers become independent and set up their own research groups. There were 491 projects funded from 2015 to 2020, usually 3 years long (with some being only 2 years long), where the PI must have been up to 8 years after their PhD defence (with potential extension of 2 years for each child the researcher had in that period) and the grant could contribute up to 100 % of their salary (usually, this is limited to 50 % in other funding programmes). From these 491 funded projects, we selected 348 (71 %) grants of the recipients from Czechia, since the remaining recipients were from abroad, and thus would not have their previous publications recorded in the database of researchers employed at Czech research institutions (RIV) that we used for all analyses. There were also 7 cases where one researcher received this grant two times - in these cases, we counted only the first project that was funded, reducing the number of recipients we worked with to 341.

In the second step, we manually searched for PhD advisors of these 341 researchers in the databases of theses and dissertations that cover most universities in Czechia (https://theses.cz and https://dspace.cuni.cz/). We were able to match `r nrow(tar_read(ids_complete))` researchers with a PhD advisor (69,2 % of the Czech subsample). Further, we have not managed to find a suitable matched pair for another `r nrow(tar_read(ids_complete))-nrow(tar_read("matching") %>% filter(treatment == 1) %>% distinct(vedidk))` researchers in the matching procedure, reducing our final sample to `r nrow(tar_read("matching") %>% filter(treatment == 1) %>% distinct(vedidk))`. 

For the matched researchers from control groups (see the matching description below in the Analysis section), we didn't search for their supervisors manually, but rather used the "most frequent co-author of the first 5 publications" as a proxy to identify supervisors. Crosschecking this proxy with the actual data about supervisors we have, we found that this algorithm was able to identify supervisors correctly in about 50 % of cases (see the Attachment 1). Using this method, we were able to find `r (nrow(tar_read("final_data") %>% filter(treatment != 1) %>% filter(!is.na(sup_name))) / nrow(tar_read("matching") %>% filter(treatment != 1)))*100` % of the observations in the matched control groups. 

These researchers and their PhD advisors with their full publication history (limited to publications created at Czech institutions) created our final sample.



## Measures

*Independence*

We decided to use the recently created independence indicator by Van den Besselaar & Sandström (2019), since it is based on scientific production (rather than impact) and captures the phenomena in the most complete manner. The indicator has 4 parts: I1: The eigenvector centrality of the supervisor in the researcher’s co-author network; I2: The clustering coefficient of the supervisor in the researcher’s co-author network; I3: The share of own papers of the researcher Pns/Pall where Pns is the number of publications of the researcher, not coauthored with the former supervisor(s) and Pall is the total number of publications of the researcher; and I4: The share of own research topics of the researcher Tns/Tall where Tns is the number of research topics of the researcher in which the former supervisor(s) is not active, and Tall is the total number of topics of the researcher.


However, we have made some changes to the indicator. After consulting with its authors, we have changed the aggregation formula to 
$$RII = ((1- I1) + I2 + I3 + I4) / 4$$, since the previous formula had a small error (it weighted I4 double,, i.e. 2*I4). For calculating the share of own research topics of the researcher (I4), we have also not used bibliographic coupling as originally suggested, but text-based topic modelling using publication titles and abstracts. Finally, the original indicator, while tracking not only collaboration independence but also topical independence, is still skewed significantly towards collaboration independence (three-quarters of the indicator measure collaboration, while only one-quarter measures topics). Since we consider cognitive/topical independence an important asset of this indicator, we have decided to adjust the formula a bit further and drop the I3 part, to give more weight to the cognitive/topical independence in the whole indicator. Throughout this paper, we will thus refer to the original indicator as RIIo, our adjusted version as RIIa and the part of the indicator measuring only the cognitive independence RIIc.

---> popsat víc jak počítám topic model??

*Career age*

We measured researchers’ career age as the number of years since their first publication.

*Discipline*

We used the highest level of the OECD classification of disciplines, containing categories Natural Sciences; Engineering and Technology; Medical and Health Sciences; Agricultural Sciences; Social Sciences; Humanities. We chose this classification because it is widely used and also used in the dataset we worked with. For the matching we used the 42 sub-categories of this classification to achieve more granularity.

We attributed a discipline to a given researcher based on disciplines attributed to their each publication, choosing the most frequent discipline for each author.  

*Interdisciplinary proportion*

This measure was calculated as the ratio of publications authored by the given researcher which were not assigned to the author’s main discipline.  

*Number of publications*

We counted all publications the given researcher produced before the intervention year (i.e. receiving a Junior Grant) while affiliated with the Czech institution (our database only contained publications affiliated with Czech institutions). We only counted publications of types: journal articles, conference proceedings, book chapters, and full books. 


*Number of publications in Web of Science and Scopus*

We counted all publications the given researcher produced before the intervention year (i.e. receiving a Junior Grant) while affiliated with the Czech institution (our database only contained publications affiliated with Czech institutions), that were also linked in Web of Science or Scopus (which means they were likely published in more recognised journals). We only counted publications of types: journal articles, conference proceedings, book chapters, and full books. 


*Number of grants received prior to Junior Grant*

We calculated how many (Czech) grants each researcher has received prior to the intervention year (i.e. receiving a Junior Grant).

*Career age when receiving the first grant*

We calculated the career age of each researcher when they received their first grant.

*Gender*

We used Genderize.io API to estimate the probability of perceived gender based on the first names of researchers. Where available, information about the nationality of researchers was included in the API call to improve accuracy.

## Analysis

We analysed the data in the R version 4.2.3. using R studio and [this code](https://github.com/david-janku/juniors). For the construction of the independence indicator, we used network modelling (maybe more specific?) and text-based topic modelling (maybe more specific??).

For the construction of the control group, we implemented propensity matching using the package MatchIt (citation?) and matching exactly by the discipline (42 sub-categories of OECD classification), and approximately by number of publications in total, number of publications in WoS/Scopus, career age, interdisciplinary proportion, number of grants received prior to Junior Grant, career age when receiving the first grant and gender. All of these variables were calculated at the intervention year (i.e. year when the given researcher from experimental group received the Junior Grant). The distance was set to "mahalanobis". The matching was done in 1:1 ratio and with replacement.  

We have created 2 control groups: one using the matching above and one similar but specifically containing only researchers who received their first grant (other than Junior Grant) around the same career age as researchers in our intervention group. That allowed us to better track the impact of any grant funding vs specifically Junior Grants funding on researchers’ independence.

The difference between groups was tested using paired t-test and subsequent differences between discipline groups was tested using ANOVA post-hoc Tukey test.

# Results 

## Descriptives

### Missing data
```{r echo=FALSE}
d <- tar_read(full_indicator)
d$disc_ford <- substring(d$disc_ford, 0, 1)
d$disc_ford <- as.factor(d$disc_ford)

na_counts <- sum(is.na(d$RII))
na_percentage <- round((na_counts / nrow(d))*100, 1)

na_sup <- round((sum(is.na(d$sup_name))/na_counts)*100, 1)
na_pubs <- round((nrow(d %>% filter(!is.na(sup_name)) %>% filter(pub_table_empty == TRUE))/na_counts)*100, 1)

na_pubs_after <- round((nrow(d %>% filter(!is.na(sup_name)) %>% filter(pub_table_empty == TRUE) %>% filter(independence_timing == "after_intervention"))/nrow(d %>% filter(!is.na(sup_name)) %>% filter(pub_table_empty == TRUE)))*100, 1)

na_treatment <- nrow(d %>% filter(is.na(RII)) %>% filter(treatment == 1))
na_control1 <- nrow(d %>% filter(is.na(RII)) %>% filter(treatment == 0))
na_control2 <- nrow(d %>% filter(is.na(RII)) %>% filter(treatment == 2))

s <- d %>% filter(is.na(RII))
                  
f_after <- d %>% filter(!is.na(RII)) %>% filter(independence_timing == "after_intervention") 
f_before <- d %>% filter(!is.na(RII)) %>% filter(independence_timing == "before_intervention") 


v_after <- c(f_after$vedidk[f_after$treatment == 1 ], NA)
v_before <- c(f_before$vedidk[f_before$treatment == 1 ], NA)

r_after <- f_after %>% filter(vedidk_treatment %in% v_after)
r_before <- f_before %>% filter(vedidk_treatment %in% v_before)



v_after_control <- c(r_after$vedidk[r_after$treatment != 1 ], r_after$vedidk_treatment)
v_before_control <- c(r_before$vedidk[r_before$treatment != 1 ], r_before$vedidk_treatment)

r_after_final <- r_after %>% filter(vedidk %in% v_after_control)
r_before_final <- r_before %>% filter(vedidk %in% v_before_control)


# r_test <- r_after %>% filter(treatment != 2)
# v_after_test <- c(r_test$vedidk[r_test$treatment != 1 ], r_test$vedidk_treatment)
# r_test_final <- r_test %>% filter(vedidk %in% v_after_test)


final_before_control1 <- nrow(r_before_final %>% filter(treatment == 0)) 
final_before_control2 <- nrow(r_before_final %>% filter(treatment == 2))


final_after_control1 <- nrow(r_after_final %>% filter(treatment == 0)) 
final_after_control2 <- nrow(r_after_final %>% filter(treatment == 2))


a <- intersect(
    r_after_final$vedidk,
    r_before_final$vedidk
)


all <- rbind(r_before_final, r_after_final) %>% filter(vedidk %in% a)

```


The independence indicator was not possible to calculate for `r na_counts` (`r na_percentage` % of observations). This is in `r na_sup` % caused by missing data about supervisors, and in `r na_pubs` % due to given researcher not having any publications in the given period (`r na_pubs_after` % of these cases are in the period after the intervention, which sometimes mean in the past few years).

There are `r na_treatment` missing observations in the treatment group, `r na_control1` in the unfunded control group and  `r na_control2` in the funded control group.
If we only keep observation that have match with non-missing values, we are left with a sample of `r final_before_control1` matched observations in unfunded control group and `r final_before_control2` matched observations in funded control group before intervention and a sample of `r final_after_control1` matched observations in unfunded control group and `r final_after_control2` matched observations in funded control group after intervention.

When Looking at disciplines, we see that there are slightly more missing data in the discipline group 1 (Natural Sciences).

```{r echo=FALSE}

dt <- d
dt$missing <- ifelse(is.na(dt$RII), "Missing", "Complete")

# Calculate counts and proportions
df_summary <- dt %>%
  group_by(disc_ford, missing) %>%
  summarise(count = n(), .groups = "drop") %>%
  mutate(Proportion = count / sum(count)) %>%
  pivot_wider(names_from = missing, values_from = c(count, Proportion))

# Calculate Missing / (Missing + Complete) in percentages for each disc_ford
df_summary <- df_summary %>%
  mutate(Proportion = ifelse(!is.na(`count_Missing`) & !is.na(`count_Complete`), `count_Missing` / (`count_Missing` + `count_Complete`) * 100, NA))

# If there are any NA values in the 'Proportion' column, replace them with 0
df_summary[is.na(df_summary$Proportion)] <- 0

# Remove unnecessary columns and rename the others
df_summary <- df_summary %>%
  select(disc_ford, count_Complete, count_Missing, Proportion) %>%
  rename(
    Complete = count_Complete,
    Missing = count_Missing,
    Proportion_Missing = Proportion
  )

# Print the updated table
kable(df_summary, digits = 3, caption = "Counts and Proportions of Missing data by disc_ford") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))



```



### Before intervention

```{r echo=FALSE}

# assuming df is your dataframe
df <- r_before_final

# list of numeric columns. Exclude treatment column
numeric_vars <- df %>% select(where(is.numeric)) %>% select(-weights, -funded_control, -id, -treatment_year, -career_start_year) %>% names()

# descriptive stats for treatment level 1
table_before_1 <- df %>% filter(treatment == 1) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 1
kable(table_before_1, digits = 3, caption = "Descriptive statistics for treatment group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 0
table_before_0 <- df %>% filter(treatment == 0) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 0
kable(table_before_0, digits = 3, caption = "Descriptive statistics for unfunded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 2
table_before_2 <- df %>% filter(treatment == 2) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 2
kable(table_before_2, digits = 3, caption = "Descriptive statistics for funded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```



### After intervention

```{r echo=FALSE}

# assuming df is your dataframe
df <- r_after_final

# list of numeric columns. Exclude treatment column
numeric_vars <- df %>% select(where(is.numeric)) %>% select(-weights, -funded_control, -id, -treatment_year, -career_start_year) %>% names()

# descriptive stats for treatment level 1
table_before_1 <- df %>% filter(treatment == 1) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 1
kable(table_before_1, digits = 3, caption = "Descriptive statistics for treatment group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 0
table_before_0 <- df %>% filter(treatment == 0) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 0
kable(table_before_0, digits = 3, caption = "Descriptive statistics for unfunded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 2
table_before_2 <- df %>% filter(treatment == 2) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 2
kable(table_before_2, digits = 3, caption = "Descriptive statistics for funded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```



## Matching

After matching, all standardized mean differences for the covariates were below 0.1 and all standardized mean differences for squares and two-way interactions between covariates were below .15, indicating adequate balance. __This removed XXXX cases from the experimental group, leaving us with YYY matched observations__.
 
 

```{r echo=FALSE}
# # Create a subset for treatment group and control group 1
# subset_1 <- (tar_read(matching)) %>% filter(treatment != 2)
# subset_2 <- (tar_read(matching)) %>% filter(treatment != 0)
# 
# # Generate balance table for treatment vs control group 1
# balance_results_1 <- bal.tab(treatment ~ career_lenght + pubs_total + ws_pubs + interdisc_proportion + grants + gender, data = subset_1, disp = c("mean", "std"))
# balance_results_2 <- bal.tab(treatment ~ career_lenght + pubs_total + ws_pubs + interdisc_proportion + grants + gender, data = subset_2, disp = c("mean", "std"))
# 
# # Define a function to manually extract balance statistics from bal.tab object
# extract_balance_stats <- function(bal_results) {
#   covariates <- rownames(bal_results$Balance)
#   means_treat <- bal_results$Balance$`M.1.Un`
#   means_ctrl <- bal_results$Balance$`M.0.Un`
#   diff <- bal_results$Balance$`Diff.Un`
#   
#   data.frame(Covariate = covariates, 
#              Mean_Treatment = means_treat, 
#              Mean_Control = means_ctrl, 
#              Diff = diff)
# }
# 
# # Extract balance stats for subset_1 and subset_2
# balance_stats_1 <- extract_balance_stats(balance_results_1)
# balance_stats_2 <- extract_balance_stats(balance_results_2)
# 
# # Print tables in R Markdown
# knitr::kable(balance_stats_1, caption = "Balance Statistics: Treatment vs Control Group 1") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
# knitr::kable(balance_stats_2, caption = "Balance Statistics: Treatment vs Control Group 2") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))




unf_sum <- summary(tar_read(matched_obj_unfunded), un = TRUE)

summary(tar_read(matched_obj_unfunded))
    
plot(unf_sum, var.order = "unmatched")


cobalt::bal.tab(tar_read(matched_obj_unfunded), un = FALSE, stats = c("m", "v", "ks"), binary = "std")

cobalt::love.plot(tar_read(matched_obj_unfunded), binary = "std", var.order = "adjusted")





fun_sum <- summary(tar_read(matched_obj_funded), un = TRUE)

summary(tar_read(matched_obj_funded))
    
plot(fun_sum, var.order = "unmatched")





```

## Main results


To estimate the treatment effect and its standard error, we fit a linear regression model with 1978 earnings as the outcome and the treatment, covariates, and their interaction as predictors and included the full matching weights in the estimation. The lm() function was used to fit the outcome, and the comparisons() function in the marginaleffects package was used to perform g-computation in the matched sample to estimate the ATT. A cluster-robust variance was used to estimate its standard error with matching stratum membership as the clustering variable.

The estimated effect was XXXX (SE = XXXX, p = XXXX), indicating that the average effect of the treatment for those who received it is to increase earnings.

```{r echo=FALSE}

# Run regression model with interaction term
did_model <- plm(researcher_independence ~ independence_timing * treatment + factor(subclass), 
                 data = tar_read(full_indicator), 
                 index = c("subclass", "independence_timing"),
                 model = "within")

# Calculate clustered standard errors
clustered_se <- coeftest(did_model, vcov = function(x) vcovHC(x, type = "HC1", cluster = "group"))
print(clustered_se)


```


### Before intervention

Was there a difference between the treatment groups in their level of independence before the intervention?

The analysis of variance (ANOVA) was used to compare the means of the three treatment groups. The results of the ANOVA showed that the effect of treatment was not statistically significant (F(2, 630) = 0.256, p = 0.613). The mean square (MS) within groups was 0.05775, which indicates that the variability within each group is relatively small. The MS between groups was 0.01476, which is smaller than the MS within groups, suggesting that the differences between the means of the treatment groups are small. The model explained only a small proportion of the total variance in the data (R-squared = 0.01). Sixty-five observations were deleted due to missing data. In conclusion, there is no evidence to suggest that the treatment has a significant effect on the outcome variable. However, the small effect size and the missing data should be taken into consideration when interpreting the results. Post-hoc test power was 0.9997622 as calculated by GPower (Faul, Erdfelder, Lang & Buchner, 2007).


```{r echo=FALSE}
set.seed(123)

d_before <- r_before_final
     
before_treatment <- aov(RII ~ treatment, data = d_before)
summary(before_treatment)

means_before_treatment <- aggregate(RII ~ treatment, data = d_before, FUN = mean)
     
     
ggplot(data = means_before_treatment, aes(x = treatment, y = RII, fill = treatment)) +
         geom_bar(stat = "identity", position = "dodge") +
         labs(x = "Treatment", y = "Mean RII") +
         scale_fill_discrete(name = "treatment") +
         theme_bw()


```

Further, were there any differences in independence across disciplines?

```{r echo=FALSE}
set.seed(123)

d_before$disc_ford <- substring(d_before$disc_ford, 0, 1)
d_before$disc_ford <- as.factor(d_before$disc_ford)
     
d_before$treatment <- as.factor(d_before$treatment)
     
table(d_before$disc_ford, by = d_before$treatment)

disc_before <- aov(RII ~ disc_ford, data = d_before)
summary(disc_before)
     
means_before_disc <- aggregate(RII ~ disc_ford, data = d_before, FUN = mean)
     
     
ggplot(data = means_before_disc, aes(x = disc_ford, y = RII, fill = disc_ford)) +
         geom_bar(stat = "identity", position = "dodge") +
         labs(x = "disc_ford", y = "Mean RII") +
         scale_fill_discrete(name = "disc_ford") +
         theme_bw()

```


Was there any interaction between the treatment group and discipline?

```{r echo=FALSE}
set.seed(123)

before_disc_interaction <- aov(RII ~ treatment*disc_ford, data = d_before)
summary(before_disc_interaction)
     
     
 means_before_disc_interaction <- aggregate(RII ~ treatment + disc_ford, data = d_before, FUN = mean)
     
     # Plot grouped bar plot
     ggplot(data = means_before_disc_interaction, aes(x = treatment, y = RII, fill = disc_ford)) +
         geom_bar(stat = "identity", position = "dodge") +
         labs(x = "Treatment", y = "Mean RII") +
         scale_fill_discrete(name = "disc_ford") +
         theme_bw()

```



### After intervention

Was there a difference between the treatment groups in their level of independence after the intervention?

The analysis of variance (ANOVA) was used to compare the means of the three treatment groups. The results of the ANOVA showed that the effect of treatment was not statistically significant (F(2, 630) = 0.256, p = 0.613). The mean square (MS) within groups was 0.05775, which indicates that the variability within each group is relatively small. The MS between groups was 0.01476, which is smaller than the MS within groups, suggesting that the differences between the means of the treatment groups are small. The model explained only a small proportion of the total variance in the data (R-squared = 0.01). Sixty-five observations were deleted due to missing data. In conclusion, there is no evidence to suggest that the treatment has a significant effect on the outcome variable. However, the small effect size and the missing data should be taken into consideration when interpreting the results. Post-hoc test power was 0.9997622 as calculated by GPower (Faul, Erdfelder, Lang & Buchner, 2007).


```{r echo=FALSE}
set.seed(123)

d_before <- r_after_final
     
before_treatment <- aov(RII ~ treatment, data = d_before)
summary(before_treatment)

means_before_treatment <- aggregate(RII ~ treatment, data = d_before, FUN = mean)
     
     
ggplot(data = means_before_treatment, aes(x = treatment, y = RII, fill = treatment)) +
         geom_bar(stat = "identity", position = "dodge") +
         labs(x = "Treatment", y = "Mean RII") +
         scale_fill_discrete(name = "treatment") +
         theme_bw()


```

Further, were there any differences in independence across disciplines?

```{r echo=FALSE}
set.seed(123)

d_before$disc_ford <- substring(d_before$disc_ford, 0, 1)
d_before$disc_ford <- as.factor(d_before$disc_ford)
     
d_before$treatment <- as.factor(d_before$treatment)
     
table(d_before$disc_ford, by = d_before$treatment)

disc_before <- aov(RII ~ disc_ford, data = d_before)
summary(disc_before)
     
means_before_disc <- aggregate(RII ~ disc_ford, data = d_before, FUN = mean)
     
     
ggplot(data = means_before_disc, aes(x = disc_ford, y = RII, fill = disc_ford)) +
         geom_bar(stat = "identity", position = "dodge") +
         labs(x = "disc_ford", y = "Mean RII") +
         scale_fill_discrete(name = "disc_ford") +
         theme_bw()

```


Was there any interaction between the treatment group and discipline?

```{r echo=FALSE}
set.seed(123)

before_disc_interaction <- aov(RII ~ treatment*disc_ford, data = d_before)
summary(before_disc_interaction)
     
     
 means_before_disc_interaction <- aggregate(RII ~ treatment + disc_ford, data = d_before, FUN = mean)
     
     # Plot grouped bar plot
     ggplot(data = means_before_disc_interaction, aes(x = treatment, y = RII, fill = disc_ford)) +
         geom_bar(stat = "identity", position = "dodge") +
         labs(x = "Treatment", y = "Mean RII") +
         scale_fill_discrete(name = "disc_ford") +
         theme_bw()

```



### Intervention effect


```{r echo=FALSE}

d <- all 
d$treatment <- as.factor(d$treatment)
d$independence_timing <- as.factor(d$independence_timing)
     
interaction <- aov(RII ~ treatment * independence_timing, data = d)
summary(interaction)
heplots::etasq(interaction, anova = TRUE)

 ###calculating r-squared - generated by chatGPT, so not sure this is correct
     
     n <- nrow(d)
     p <- length(coef(interaction))
     SSresid <- sum(resid(interaction)^2)
     SStotal <- sum((na.omit(d$RII) - mean(d$RII, na.rm = TRUE))^2)
     R2 <- 1 - (SSresid / SStotal)
     adjR2 <- 1 - ((1 - R2) * (n - 1) / (n - p - 1))

 ### visualisations
     
     
means <- aggregate(RII ~ treatment + independence_timing, data = d, FUN = mean)

 ggplot(data = interaction, aes(x = treatment, y = RII, fill = independence_timing)) +
         geom_violin(scale = "width") +
         scale_fill_discrete(name = "Independence Timing", labels = c("After intervention", "Before intervention")) +
         xlab("Treatment Group") +
         ylab("RII Score") +
         ggtitle("Violin Plot of RII Scores by Treatment and Time") +
         theme_bw()
 
 
```


# Discussion



## Limitations

--> the identification of supervisors in the control group (only 50% correct)
--> only talks about the Czech environment and considers researchers growing up and most of their careers in the Czech republic

# Attachments

## Identification of supervisors

Data about PhD student-PhD advisor connections are usually not available. One must search for these connections manually on the internet, which is time consuming and not fully reliable (we were only able to find about 69 % of these connections in our treatment group). Therefore, we tried to come up with an alternative way of identifying supervisors from the available data: we assumed that since many researchers only start their publication career during their PhD studies, their PhD supervisors might often coauthor their first publications. That led us to creating an algorithm to identify the supervisor solely from the student's publication history as "the most frequent co-author of the first 5 publications".

To check whether we can use "the most frequent co-author of the first 5 publications" as a proxy to identify supervisors, we have compared the supervisors identified via this method with the manually found supervisors for each researcher in our sample.
In the treatment group of `r nrow(tar_read(ids_complete))`researchers,  118 (49 %) had their most frequent co-author same as their supervisor and 86 (36 %) had someone other than their supervisor as their most frequent co-author in their first 5 publications, and 37 (15%) had no co-author in their first 5 publications.  
In the control groups, we have manually found supervisors for `r nrow(tar_read(sup_control) %>% filter(!is.na(sup_vedidk)) %>% distinct(name_first, name_last))` researchers, and when we compared them to the supervisors suggested by the most frequent coauthor algorithm, we found that 41 (29 %) had their most frequent co-author the same as their supervisor, 80 (57 %) had someone other than their supervisor as their most frequent co-author in their first 5 publications, and 20 (14 %) had no co-author in their first 5 publications.

Further, we have conducted analysis on an independent sample of researchers from Slovenia (citace Luzar??), and found that  when we filter out all researchers who have not had any publication 4 years prior to their PhD defense (1135, approx 9,5% of the sample), we see that from the remaining researchers, about 50,3 % had their supervisor as the most frequent coauthor in the 4 years prior to their PhD defense.



# References


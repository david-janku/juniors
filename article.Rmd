---
title: "Funding programmes for early career researchers do/not influence their independence"
author: "David Janků"
date: "`r Sys.Date()`"
output:
  html_document: default
  word_document: default
bibliography: researcher_independence.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(targets)
library(tidyr)
library(dplyr)
library(cobalt)
library(knitr)
library(psych)
library(kableExtra)
library(ggplot2)
library(lme4)
library(lmerTest)
library(MuMIn)
library(ez)
library(plm)

```

# Intro

Researchers’ independence from their PhD advisor is one of the criteria often used for assessing researchers for hiring, tenure and promotion (Van Arensbergen, Van der Weijden, Van den Besselaar, 2014; Moher et al, 2018), constituting one factor of research excellence.
The promise of independence is that researchers will “challenge the existing common views and open up new lines of research” (Van den Besselaar & Sandström, 2019, p.2). 

An independent investigator is then defined by the US National Academies of Science (NAS) as “one who enjoys independence of thought—the freedom to define the problem of interest and to choose or develop the best strategies and approaches to address that problem–especially during the earlier career phase” (Van den Besselaar & Sandström, 2019, p.4). Such a definition does not necessarily put independence in relation to other actors (e.g. one’s PhD advisor), but only in relation to one’s own freedom and resources. However other institutions, such as European Research Council via their Starting Grants scheme (Neufeld, Huber & Wegner, 2013) think of independence in a relational manner, in terms of having one (or a few) publications without the former PhD supervisor being co-author; being PI on a grant; or being lab director or research leader. 

Based on some early empirical studies (Rojko & Lužar, 2022), it seems that researchers in disciplines such as philosophy, psychology; religion, theology; social sciences; the arts, recreation, entertainment, sport; language, linguistics, literature; and geography, biography, history experience significantly higher levels of indepedence on their PhD advisor than researchers in disciplines such as science and knowledge, organization, computer science, information, documentation, librarianship, institutions, publications; mathematics, natural sciences; and applied sciences, medicine, technology. 
There is also some preliminary evidence that higher indepedence modestly correlates with number of citations aquired (Rojko & Lužar, 2022), but others argue that even the research systems with very low indepedence can create a lot of scientific impact, and thus the relationships between independence and impact might not be as clear (Gläser et al., 2021)


In recent decades, there has been a worry that researchers’ indepedence is decreasing, illustrated by increase in age at which US (biomedical) researchers received their first grant (NAS, 2005; Daniels, 2015), as well as direct measures of new reserchers becoming more depedent on their PhD advisors across almost all disciplines in Slovenia (Rojko & Lužar, 2022, table 6 on p.9). Perhaps to tackle this trend, many grant agencies decided to start awarding grants for early career researchers, usually up to 8 years after finishing their PhD. Such programmes have often explicitly stated goals of helping researchers become more indepedent, set up their own research group and open up new lines of research. The examples of such programmes could be Starting Grants by European Research Council (2022), Junior project and Junior Star by Czech Science Foundation (2020), Walter N. Benjamin Programme by DFG (2022), Innovation Research Incentives Scheme by Netherlands Organization of Scientific Research (NWO) (2021)... something from the US would be also good)


Although researcher independence seems to be important feature that is acknowledged by many actors in the research systems and that research funders try to stimulate, there hasn’t been much effort focused on quantifying this phenomenon. Some studies try to measure independence in a simple way, as a ratio of publications the given researcher produced without their supervisor divided by the total number of publications of the given researchers (e.g. Rojko & Lužar, 2022). However, such indicator could be misleading in the world of team science, and also only focuses on collaborations, not tracking whether given researcher become independent in their thinking and research topics. [@vandenbesselaarMeasuringResearcherIndependence2019] therefore proposed more complex indicator for measuring researcher independence on the individual level, based on co-authorship network and topical overlaps. Their measure uses only information about publications’ characteristics (authorship and reference list) regardless of it’s impacts, unlike other alternative measures that focus solely on the impact of the independent work in terms of received citations (Dey, et al., 2021).

Since the explicit goal of many funding instruments that are being used by research funders all over the world is to help early career researchers become independent, we would like to use the newly created indicators to assess the extent, to which such programmes influence researchers independence.    

H1: Researchers, who received funding from programmes aimed at funding early career researchers, will show more independence on their PhD advisor 5 years after receiving the funding than researchers who did not receive this kind of funding.

# Method

## Sample / data

We used the database of all publications produced by researchers employed at Czech research institutions (RIV) combined with a database of all publicly funded research projects and grants in the Czech Republic (CEP), both publicly accessible at https://www.isvavai.cz/, and transformed into a single database with cleaned data, unique person and publication identifiers, and better structure (Hladik, XXX).
This allowed us to identify all researchers who received Junior Grants from the Czech Science Foundation, a funding programme that has an explicit goal to help early career researchers become independent and set up their own research groups. There were 491 projects funded from 2015 to 2020, usually 3 years long (with some being only 2 years long), where the PI must have been up to 8 years after their PhD defence (with potential extension of 2 years for each child the researcher had in that period) and the grant could contribute up to 100 % of their salary (usually, this is limited to 50 % in other funding programmes). From these 491 funded projects, we selected 348 (71 %) grants of the recipients from Czechia, since the remaining recipients were from abroad, and thus would not have their previous publications recorded in the database of researchers employed at Czech research institutions (RIV) that we used for all analyses. There were also 7 cases where one researcher received this grant two times - in these cases, we counted only the first project that was funded, reducing the number of recipients we worked with to 341.

In the second step, we manually searched for PhD advisors of these 341 researchers in the databases of theses and dissertations that cover most universities in Czechia (https://theses.cz and https://dspace.cuni.cz/). We were able to match `r nrow(tar_read(ids_complete))` researchers with a PhD advisor (69,2 % of the Czech subsample). Further, we have not managed to find a suitable matched pair for another `r nrow(tar_read(ids_complete))-nrow(tar_read("matching") %>% filter(treatment == 1) %>% distinct(vedidk))` researchers in the matching procedure, reducing our final sample to `r nrow(tar_read("matching") %>% filter(treatment == 1) %>% distinct(vedidk))`. 

For the matched researchers from control groups (see the matching description below in the Analysis section), we didn't search for their supervisors manually, but rather used the "most frequent co-author of the first 5 publications" as a proxy to identify supervisors. Crosschecking this proxy with the actual data about supervisors we have, we found that this algorithm was able to identify supervisors correctly in about 50 % of cases (see the Attachment 1). Using this method, we were able to find `r (nrow(tar_read("final_data") %>% filter(treatment != 1) %>% filter(!is.na(sup_name))) / nrow(tar_read("matching") %>% filter(treatment != 1)))*100` % of the observations in the matched control groups. 

These researchers and their PhD advisors with their full publication history (limited to publications created at Czech institutions) created our final sample.



## Measures

*Independence*

We decided to use the recently created independence indicator by Van den Besselaar & Sandström (2019), since it is based on scientific production (rather than impact) and captures the phenomena in the most complete manner. The indicator has 4 parts: I1: The eigenvector centrality of the supervisor in the researcher’s co-author network; I2: The clustering coefficient of the supervisor in the researcher’s co-author network; I3: The share of own papers of the researcher Pns/Pall where Pns is the number of publications of the researcher, not coauthored with the former supervisor(s) and Pall is the total number of publications of the researcher; and I4: The share of own research topics of the researcher Tns/Tall where Tns is the number of research topics of the researcher in which the former supervisor(s) is not active, and Tall is the total number of topics of the researcher.


However, we have made some changes to the indicator. After consulting with its authors, we have changed the aggregation formula to 
$$RII = ((1- I1) + I2 + I3 + I4) / 4$$, since the previous formula had a small error (it weighted I4 double,, i.e. 2*I4). For calculating the share of own research topics of the researcher (I4), we have also not used bibliographic coupling as originally suggested, but text-based topic modelling using publication titles and abstracts. Finally, the original indicator, while tracking not only collaboration independence but also topical independence, is still skewed significantly towards collaboration independence (three-quarters of the indicator measure collaboration, while only one-quarter measures topics). Since we consider cognitive/topical independence an important asset of this indicator, we have decided to adjust the formula a bit further and drop the I3 part, to give more weight to the cognitive/topical independence in the whole indicator. Throughout this paper, we will thus refer to the original indicator as RIIo, our adjusted version as RIIa and the part of the indicator measuring only the cognitive independence RIIc.

---> popsat víc jak počítám topic model??

*Career age*

We measured researchers’ career age as the number of years since their first publication.

*Discipline*

We used the highest level of the OECD classification of disciplines, containing categories Natural Sciences; Engineering and Technology; Medical and Health Sciences; Agricultural Sciences; Social Sciences; Humanities. We chose this classification because it is widely used and also used in the dataset we worked with. For the matching we used the 42 sub-categories of this classification to achieve more granularity.

We attributed a discipline to a given researcher based on disciplines attributed to their each publication, choosing the most frequent discipline for each author.  

*Interdisciplinary proportion*

This measure was calculated as the ratio of publications authored by the given researcher which were not assigned to the author’s main discipline.  

*Number of publications*

We counted all publications the given researcher produced before the intervention year (i.e. receiving a Junior Grant) while affiliated with the Czech institution (our database only contained publications affiliated with Czech institutions). We only counted publications of types: journal articles, conference proceedings, book chapters, and full books. 


*Number of publications in Web of Science and Scopus*

We counted all publications the given researcher produced before the intervention year (i.e. receiving a Junior Grant) while affiliated with the Czech institution (our database only contained publications affiliated with Czech institutions), that were also linked in Web of Science or Scopus (which means they were likely published in more recognised journals). We only counted publications of types: journal articles, conference proceedings, book chapters, and full books. 


*Number of grants received prior to Junior Grant*

We calculated how many (Czech) grants each researcher has received prior to the intervention year (i.e. receiving a Junior Grant).

*Career age when receiving the first grant*

We calculated the career age of each researcher when they received their first grant.

*Gender*

We used Genderize.io API to estimate the probability of perceived gender based on the first names of researchers. Where available, information about the nationality of researchers was included in the API call to improve accuracy. 
We have also manually specified gender for 2 obrsevtation from the treatment group for which the gender was not calculated automatically. 

## Analysis

We analysed the data in the R version 4.2.3. using R studio and [this code](https://github.com/david-janku/juniors). For the construction of the independence indicator, we used network modelling (maybe more specific?) and text-based topic modelling (maybe more specific??).

For the construction of the control group, we implemented propensity matching using the package MatchIt (citation?) and matching exactly by the discipline (42 sub-categories of OECD classification) and treatment year, and approximately by number of publications in total, number of publications in WoS/Scopus, career age, interdisciplinary proportion, number of grants received prior to Junior Grant, career age when receiving the first grant and gender. All of these variables were calculated at the intervention year (i.e. year when the given researcher from experimental group received the Junior Grant). The distance was set to "mahalanobis". The matching was done in 1:3 ratio and with replacement. Manual search for supervisors of the matched control group observations was done by ranking the control group observation by distance from the treatment group observation and first searching for the supervisor of the closest control group observation, and if it was not possible to find it, then repeat the same for the second closest, etc... In the end each treatment observation was matched with exactly one control group observation which was closest but at the same time for which it was possible to identify their supervisor.       

We have created 2 control groups: one using the matching above and one similar but specifically containing only researchers who received their first grant (other than Junior Grant) around the same career age as researchers in our intervention group. That allowed us to better track the impact of any grant funding vs specifically Junior Grants funding on researchers’ independence.

The difference between groups was tested using paired t-test and subsequent differences between discipline groups was tested using ANOVA post-hoc Tukey test.

# Results 

## Descriptives

### Missing data
```{r echo=FALSE}
d <- tar_read(full_indicator)
d$disc_ford <- as.factor(d$disc_ford)

na_counts <- sum(is.na(d$RII))
na_percentage <- round((na_counts / nrow(d))*100, 1)

na_sup <- round((sum(is.na(d$sup_name))/na_counts)*100, 1)
na_pubs <- round((nrow(d %>% filter(!is.na(sup_name)) %>% filter(is.na(pubs_number)))/na_counts)*100, 1)

na_pubs_after <- round((nrow(d %>% filter(!is.na(sup_name)) %>% filter(is.na(pubs_number)) %>% filter(independence_timing == "after_intervention"))/nrow(d %>% filter(!is.na(sup_name)) %>% filter(is.na(pubs_number))))*100, 1)

na_treatment <- nrow(d %>% filter(is.na(RII)) %>% filter(treatment == 1))
na_control1 <- nrow(d %>% filter(is.na(RII)) %>% filter(treatment == 0))
na_control2 <- nrow(d %>% filter(is.na(RII)) %>% filter(treatment == 2))

# s <- d %>% filter(is.na(RII))
#                   
# f_after <- d %>% filter(!is.na(RII)) %>% filter(independence_timing == "after_intervention") 
# f_before <- d %>% filter(!is.na(RII)) %>% filter(independence_timing == "before_intervention") 
# 
# 
# v_after <- c(f_after$vedidk[f_after$treatment == 1 ], NA)
# v_before <- c(f_before$vedidk[f_before$treatment == 1 ], NA)
# 
# r_after <- f_after %>% filter(vedidk_treatment %in% v_after)
# r_before <- f_before %>% filter(vedidk_treatment %in% v_before)
# 
# 
# 
# v_after_control <- c(r_after$vedidk[r_after$treatment != 1 ], r_after$vedidk_treatment)
# v_before_control <- c(r_before$vedidk[r_before$treatment != 1 ], r_before$vedidk_treatment)
# 
# r_after_final <- r_after %>% filter(vedidk %in% v_after_control)
# r_before_final <- r_before %>% filter(vedidk %in% v_before_control)


# r_test <- r_after %>% filter(treatment != 2)
# v_after_test <- c(r_test$vedidk[r_test$treatment != 1 ], r_test$vedidk_treatment)
# r_test_final <- r_test %>% filter(vedidk %in% v_after_test)


# final_before_control1 <- nrow(r_before_final %>% filter(treatment == 0)) 
# final_before_control2 <- nrow(r_before_final %>% filter(treatment == 2))
# 
# 
# final_after_control1 <- nrow(r_after_final %>% filter(treatment == 0)) 
# final_after_control2 <- nrow(r_after_final %>% filter(treatment == 2))


# a <- intersect(
#     r_after_final$vedidk,
#     r_before_final$vedidk
# )
# 
# 
# all <- rbind(r_before_final, r_after_final) %>% filter(vedidk %in% a)


filtered_by_subclass <- d %>%
     group_by(subclass, independence_timing) %>%
     filter(!any(is.na(RII))) %>%
     ungroup()

filtered_subclass <- nrow(filtered_by_subclass %>% distinct(subclass))
subclass_all <- nrow(d %>% distinct(subclass))
filtered_subclass_perc <- round((filtered_subclass/subclass_all)*100, 1)


final_before_control1 <- nrow(filtered_by_subclass %>% filter(independence_timing == "before_intervention") %>% filter(treatment == 0))
final_before_control2 <- nrow(filtered_by_subclass %>% filter(independence_timing == "before_intervention") %>% filter(treatment == 2))

final_after_control1 <- nrow(filtered_by_subclass %>% filter(independence_timing == "after_intervention") %>% filter(treatment == 0))
final_after_control2 <- nrow(filtered_by_subclass %>% filter(independence_timing == "after_intervention") %>% filter(treatment == 2))


```


The independence indicator was not possible to calculate for `r na_counts` (`r na_percentage` % of observations). This is in `r na_sup` % caused by missing data about supervisors, and in `r na_pubs` % due to given researcher not having any publications in the given period (`r na_pubs_after` % of these cases are in the period after the intervention, which sometimes mean in the past few years).

There are `r na_treatment` missing observations in the treatment group, `r na_control1` in the unfunded control group and  `r na_control2` in the funded control group.
If we only keep observations that have a match with non-missing values, we are left with a sample of `r final_before_control1` matched observations in unfunded control group and `r final_before_control2` matched observations in funded control group before intervention and a sample of `r final_after_control1` matched observations in unfunded control group and `r final_after_control2` matched observations in funded control group after intervention.

When Looking at disciplines, we see that there are slightly more missing data in Agricultural and veterinary scieces and the Humanities and the Arts.

```{r echo=FALSE}

dt <- d
dt$missing <- ifelse(is.na(dt$RII), "Missing", "Complete")

# Calculate counts and proportions
df_summary <- dt %>%
  group_by(field, missing) %>%
  summarise(count = n(), .groups = "drop") %>%
  mutate(Proportion = count / sum(count)) %>%
  pivot_wider(names_from = missing, values_from = c(count, Proportion))

# If there are any NA values in the 'count_Missing' column, replace them with 0
df_summary$count_Missing[is.na(df_summary$count_Missing)] <- 0

# Calculate Missing / (Missing + Complete) in percentages for each disc_ford
df_summary <- df_summary %>%
  mutate(Proportion = ifelse(!is.na(`count_Missing`) & !is.na(`count_Complete`), `count_Missing` / (`count_Missing` + `count_Complete`) * 100, NA))

# If there are any NA values in the 'Proportion' column, replace them with 0
df_summary$Proportion[is.na(df_summary$Proportion)] <- 0

# Remove unnecessary columns and rename the others
df_summary <- df_summary %>%
  select(field, count_Complete, count_Missing, Proportion) %>%
  rename(
    Complete = count_Complete,
    Missing = count_Missing,
    Proportion_Missing = Proportion
  )

# Print the updated table
kable(df_summary, digits = 3, caption = "Counts and Proportions of Missing data by disc_ford") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))



```


### Before intervention

POZN: here we show data 

```{r echo=FALSE}

# assuming df is your dataframe
df <- filtered_by_subclass %>% filter(independence_timing == "before_intervention")

# list of numeric columns. Exclude treatment column
numeric_vars <- df %>% select(where(is.numeric)) %>% select(-weights, -funded_control, -id, -treatment_year, -career_start_year) %>% names()

# descriptive stats for treatment level 1
table_before_1 <- df %>% filter(treatment == 1) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 1
kable(table_before_1, digits = 3, caption = "Descriptive statistics for treatment group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 0
table_before_0 <- df %>% filter(treatment == 0) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 0
kable(table_before_0, digits = 3, caption = "Descriptive statistics for unfunded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 2
table_before_2 <- df %>% filter(treatment == 2) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 2
kable(table_before_2, digits = 3, caption = "Descriptive statistics for funded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```



### After intervention

```{r echo=FALSE}

# assuming df is your dataframe
df <- filtered_by_subclass %>% filter(independence_timing == "after_intervention")

# list of numeric columns. Exclude treatment column
numeric_vars <- df %>% select(where(is.numeric)) %>% select(-weights, -funded_control, -id, -treatment_year, -career_start_year) %>% names()

# descriptive stats for treatment level 1
table_before_1 <- df %>% filter(treatment == 1) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 1
kable(table_before_1, digits = 3, caption = "Descriptive statistics for treatment group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 0
table_before_0 <- df %>% filter(treatment == 0) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 0
kable(table_before_0, digits = 3, caption = "Descriptive statistics for unfunded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

# descriptive stats for treatment level 2
table_before_2 <- df %>% filter(treatment == 2) %>% select(all_of(numeric_vars)) %>% describe() %>% select(n, mean, sd, min, max) %>% round(., digits = 3)

# Print table for treatment level 2
kable(table_before_2, digits = 3, caption = "Descriptive statistics for funded control group") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```



## Matching

```{r echo=FALSE}

removed_treatment_unfunded <- (tar_read(ids_complete) %>% distinct(vedidk) %>% nrow())-(tar_read(matching) %>% group_by(subclass) %>% filter(!any(treatment == 2)) %>%
  ungroup() %>% filter(treatment == "1") %>% distinct(vedidk) %>% nrow()) 

removed_treatment_funded <- (tar_read(ids_complete) %>% distinct(vedidk) %>% nrow())-(tar_read(matching) %>% group_by(subclass) %>% filter(!any(treatment == 0)) %>%
  ungroup() %>% filter(treatment == "1") %>% distinct(vedidk) %>% nrow()) 

unique_treatment_unfunded <- d %>% group_by(subclass) %>% filter(!any(treatment == 2)) %>% ungroup() %>% filter(treatment == "1") %>% distinct(vedidk) %>% nrow()

unique_treatment_funded <- d %>% group_by(subclass) %>% filter(!any(treatment == 0)) %>%
  ungroup() %>% filter(treatment == "1") %>% distinct(vedidk) %>% nrow()

unique_control_unfunded <- d %>% filter(treatment == 0) %>% distinct(vedidk) %>% nrow()
    
unique_control_funded <- d %>% filter(treatment == 2) %>% distinct(vedidk) %>% nrow()
    

replaced_control_unfunded <- unique_treatment_unfunded-unique_control_unfunded 

replaced_control_funded <- unique_treatment_funded-unique_control_funded 


overlap_control <- unique_control_unfunded+unique_control_funded-(d %>% filter(treatment != 1) %>% distinct(vedidk) %>% nrow())


```


After matching, all standardized mean differences for the covariates were below 0.1 and all standardized mean differences for squares and two-way interactions between covariates were below .15, indicating adequate balance. 

Matching process removed `r removed_treatment_unfunded` treatment observations from matching with unfunded control group and `r removed_treatment_funded` treatment observations from matching with funded control group. This observation was removed because the researchers had no publications prior to the year of application for the grant, which would make it not possible to calculate the independence score for them anyway.

The resulting sample had `r unique_treatment_unfunded` unique treatment group observations paired with `r unique_control_unfunded` unique unfunded control group observations suggesting that `r replaced_control_unfunded` observations from unfunded control group were matched to multiple treatment group observations. Similarly, there were `r unique_treatment_funded` unique treatment group observations paired with `r unique_control_funded` unique funded control group observations, suggesting that `r replaced_control_funded` funded control group observations were matched to multiple treatment group observations.
 
BTW: there is an overlap between control group observations - `r overlap_control` observations from the unfunded control group also appears in the funded control group
 

```{r echo=FALSE}
# # Create a subset for treatment group and control group 1
# subset_1 <- (tar_read(matching)) %>% filter(treatment != 2)
# subset_2 <- (tar_read(matching)) %>% filter(treatment != 0)
# 
# # Generate balance table for treatment vs control group 1
# balance_results_1 <- bal.tab(treatment ~ career_lenght + pubs_total + ws_pubs + interdisc_proportion + grants + gender, data = subset_1, disp = c("mean", "std"))
# balance_results_2 <- bal.tab(treatment ~ career_lenght + pubs_total + ws_pubs + interdisc_proportion + grants + gender, data = subset_2, disp = c("mean", "std"))
# 
# # Define a function to manually extract balance statistics from bal.tab object
# extract_balance_stats <- function(bal_results) {
#   covariates <- rownames(bal_results$Balance)
#   means_treat <- bal_results$Balance$`M.1.Un`
#   means_ctrl <- bal_results$Balance$`M.0.Un`
#   diff <- bal_results$Balance$`Diff.Un`
#   
#   data.frame(Covariate = covariates, 
#              Mean_Treatment = means_treat, 
#              Mean_Control = means_ctrl, 
#              Diff = diff)
# }
# 
# # Extract balance stats for subset_1 and subset_2
# balance_stats_1 <- extract_balance_stats(balance_results_1)
# balance_stats_2 <- extract_balance_stats(balance_results_2)
# 
# # Print tables in R Markdown
# knitr::kable(balance_stats_1, caption = "Balance Statistics: Treatment vs Control Group 1") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
# knitr::kable(balance_stats_2, caption = "Balance Statistics: Treatment vs Control Group 2") %>% kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))




unf_sum <- summary(tar_read(matched_obj_unfunded), un = TRUE)

summary(tar_read(matched_obj_unfunded))
    
plot(unf_sum, var.order = "unmatched")


cobalt::bal.tab(tar_read(matched_obj_unfunded), un = FALSE, stats = c("m", "v", "ks"), binary = "std")

cobalt::love.plot(tar_read(matched_obj_unfunded), binary = "std", var.order = "adjusted")





fun_sum <- summary(tar_read(matched_obj_funded), un = TRUE)

summary(tar_read(matched_obj_funded))
    
plot(fun_sum, var.order = "unmatched")





```

## Main results

To ascertain the differences in independence scores before and after intervention, while accounting for the treatment type, academic disciplines, and paired nature of the data, we used repeated measures ANOVA. This allowed us to measure the main effect of change in independence across time, across disciplines, and the main treatment effect was the interaction between treatment type and time.


POZN: only for comparison:
To estimate the treatment effect and its standard error, we fit a linear regression model with 1978 earnings as the outcome and the treatment, covariates, and their interaction as predictors and included the full matching weights in the estimation. The lm() function was used to fit the outcome, and the comparisons() function in the marginaleffects package was used to perform g-computation in the matched sample to estimate the ATT. A cluster-robust variance was used to estimate its standard error with matching stratum membership as the clustering variable.

The estimated effect was XXXX (SE = XXXX, p = XXXX), indicating that the average effect of the treatment for those who received it is to increase earnings.

```{r echo=FALSE}

pp <- d %>%
     group_by(subclass) %>%
     filter(!any(is.na(RII))) %>%
     ungroup()

filtered_by_subclass_1 <- pp %>% 
    group_by(subclass) %>% 
    filter(!any(treatment == 2)) %>% 
    ungroup()

filtered_by_subclass_1$subclass <- as.factor(filtered_by_subclass_1$subclass)
filtered_by_subclass_1$treatment <- as.factor(filtered_by_subclass_1$treatment)
filtered_by_subclass_1$independence_timing <- as.factor(filtered_by_subclass_1$independence_timing)
filtered_by_subclass_1$field <- as.factor(filtered_by_subclass_1$field)

# this DiD model unfortunately doesnt work for some reason that I cannot resolve, but it is possible to calculate the same using repeated measures anova or mixes effects model, so it should be fine

# # Run regression model with interaction term
# did_model <- plm::plm(RII ~ independence_timing * treatment + factor(subclass),
#                  data = filtered_by_subclass_1,
#                  index = c("subclass"),
#                  model = "within")
# 
# # Calculate clustered standard errors
# clustered_se <- coeftest(did_model, vcov = function(x) vcovHC(x, type = "HC1", cluster = "subclass"))




## here should be some assumptions tests that I will run to see whtehr it is correct, but later I will comment it so that it doesnt show in the final paper


model <- lmer(RII ~ treatment * independence_timing + (1 | subclass), data = filtered_by_subclass_1)
summary(model)
r.squaredGLMM(model)



##checking assumptiosn fro repeated measures ANOVA

# Normality: You can check the normality of the residuals using a Q-Q plot and the Shapiro-Wilk test.

model_lm <- lm(RII ~ treatment + subclass + subclass:independence_timing, data = filtered_by_subclass_1)

# Check residuals from lm
plot(resid(model_lm))

#Linearity and Homoscedasticity (Equal Variances) of Residuals: If the assumptions hold, you should not see any obvious patterns or funnel shapes in the plot.

plot(model_lm$fitted.values, resid(model_lm), 
     xlab = "Fitted values", ylab = "Residuals", 
     main = "Residuals vs. Fitted Values")
abline(h = 0, col = "red")

# Normality of Residuals: 
qqnorm(resid(model_lm))
qqline(resid(model_lm))

shapiro.test(resid(model_lm)) 


# Independence of Residuals: If your data has a time sequence (e.g., repeated measurements over time), then you would be concerned about the independence of residuals. For this, you can use the Durbin-Watson test from the car package. The Durbin-Watson test statistic ranges from 0 to 4. A value around 2 suggests no autocorrelation, while values below 1 and above 3 are cause for concern.

library(car)
durbinWatsonTest(model_lm)


# Multicollinearity: Check the variance inflation factor (VIF) for each predictor. Typically, a VIF above 5-10 indicates multicollinearity.

vif(model_lm)

# Outliers and Influence Points: Leverage vs. studentized residuals plot can be used to identify influential points.

plot(hatvalues(model_lm), rstudent(model_lm),
     xlab = "Leverage", ylab = "Studentized Residuals",
     main = "Residuals vs. Leverage")
abline(h = c(-2,2), col = "red", lty = 2)



# Sphericity: Use the mauchly.test function on your model. If the test is significant, then the assumption of sphericity has been violated.

# mauchly.test(model_aov)

# Homogeneity of Variance: Use Levene's Test. The car package offers leveneTest.

library(car)
leveneTest(RII ~ treatment, data = filtered_by_subclass_1)

# Outliers:You can inspect boxplots or use Mahalanobis distance.

boxplot(RII ~ treatment, data = filtered_by_subclass_1)



##conducting repeated measures ANOVA


result <- ezANOVA(data = filtered_by_subclass_1,
                 dv = RII,
                 wid = subclass,
                 within = .(independence_timing),
                 between = treatment)
print(result)



## adding disciplines:


result_disc <- ezANOVA(data = filtered_by_subclass_1,
                 dv = RII,
                 wid = subclass,
                 within = .(independence_timing),
                 between = .(treatment, field))
print(result_disc)









### visualisations
     
     
means <- aggregate(RII ~ treatment + independence_timing, data = filtered_by_subclass_1, FUN = mean)

 ggplot(data = filtered_by_subclass_1, aes(x = treatment, y = RII, fill = independence_timing)) +
         geom_violin(scale = "width") +
         scale_fill_discrete(name = "Independence Timing", labels = c("Before intervention", "After intervention")) +
         xlab("Treatment Group") +
         ylab("RII Score") +
         ggtitle("Violin Plot of RII Scores by Treatment and Time") +
         theme_bw()


means_disc <- aggregate(RII ~ field, data = filtered_by_subclass_1, FUN = mean)
     
num_disc <- filtered_by_subclass_1 %>%
  group_by(field) %>%
  summarise(count = n(), .groups = "drop")

means_disc <- left_join(means_disc, num_disc, by = "field") 

# Step 1: Reorder the levels of 'field'
means_disc$field <- with(means_disc, factor(field, levels=unique(field[order(RII)])))

# Step 2: Optionally, sort the dataframe based on the reordered levels
means_disc <- means_disc %>% arrange(field)

    
ggplot(data = means_disc, aes(x = reorder(c(1:6), RII), y = RII, fill = field)) +
  geom_bar(stat = "identity", position = "dodge") +
  
  # Add the number of observations on top of the bars
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.5) +

  labs(x = "Field", y = "Mean RII") +
  scale_fill_discrete(name = "Field") +
  theme_bw()

 
 
```




In the first analysis, we examined the effects of treatment, independence_timing, and their interaction on the dependent variable, researcher independence (RII) for the treatment group and unfunded control group. There was a significant main effect of independence_timing on RII, F(`r result$ANOVA[2, 2]`, `r result$ANOVA[2, 3]`) = `r result$ANOVA[2, 4]`, p = `r result$ANOVA[2, 5]` indicating a significant change in the independence from before to after the intervention. The effect size (GES) for this effect was `r result$ANOVA[2, 7]`, suggesting a moderate effect.
Importantly, there was a significant interaction effect between treatment and independence_timing on independence, F(`r result$ANOVA[3, 2]`, `r result$ANOVA[3, 3]`) = `r result$ANOVA[3, 4]`, p = `r result$ANOVA[3, 5]`. This interaction suggests that the effect of the intervention on independence differed based on the treatment group. The effect size (GES) for this interaction was `r result$ANOVA[3, 7]`, indicating a small effect.

In the subsequent analysis, we further incorporated the factor of discipline (field) to explore if there were differences across disciplines in the observed effects. There was a significant main effect of field on independence, F(`r result_disc$ANOVA[2, 2]`, `r result_disc$ANOVA[2, 3]`) = `r result_disc$ANOVA[2, 4]`, p = `r result_disc$ANOVA[2, 5]`, indicating that there were significant differences in RII across the six discipline groups. On average, Agricultural and veterinary sciences showed lowest independence, while Social Sciences and Humanities showed almost twice as large independence, (though it is important to note that small sample might provide unstable estimates (----> i SHOULD REMOVE THEM FROM THE ANALYSIS SINCE THESE SEEM TO BE TOO FEW OBSERVATIONS --> MAYBE I SHOULD ALSO DO A SEPARATE ANOVA ONLY TAKING INTO ACCOUNT ALL DISC BEFORE INTEVENTION AND AFTER INTERVENTION SEPARATELY BUT FOR BOTH CONTROL GROUPS). The effect size (GES) for this was `r result_disc$ANOVA[2, 7]`, suggesting a moderate effect. The interaction between treatment and field was not statistically significant, F(`r result_disc$ANOVA[4, 2]`, `r result_disc$ANOVA[4, 3]`) = `r result_disc$ANOVA[4, 4]`, p = `r result_disc$ANOVA[4, 5]`, suggesting that the effect of treatment on independence was consistent across disciplines. The three-way interaction among treatment, independence_timing, and field was not statistically significant, F(`r result_disc$ANOVA[6, 2]`, `r result_disc$ANOVA[6, 3]`) = `r result_disc$ANOVA[6, 4]`, p = `r result_disc$ANOVA[6, 5]`. This implies that the interaction effect of treatment and independence_timing on independence remained consistent across different disciplines.



POZN ON MODEL CHOICE:

In Conclusion:

Both the mixed-effects model and the RM-ANOVA suggest a significant interaction between the treatment and the timing of the intervention. This means that the effect of the intervention differed between the treatment and control groups.

Both methods also indicate that the timing of the intervention (i.e., whether it's before or after) has a significant effect on the outcome RII.

The models are in agreement with their primary findings, which is a good sign. It suggests that the treatment has a distinct effect on RII after the intervention, different from that of the control group.

However, it's worth noting that while the methods reach similar conclusions, they approach the problem from different angles. The mixed-effects model incorporates the correlation within each subclass through its random effect, while the RM-ANOVA primarily focuses on the repeated measures nature of the data. Both models provide valuable insights, but the mixed-effects model might be a more robust choice for this particular dataset, given its explicit modeling of the subclass structure.


My take:


I understand the results of ANOVa, but dont understand the results of the mixed effects model very much. I also like that ANOVa actually provides p statistic and effect size statistic


```{r echo=FALSE}


filtered_by_subclass_2 <- pp %>% 
    group_by(subclass) %>% 
    filter(!any(treatment == 0)) %>% 
    ungroup()

filtered_by_subclass_2$subclass <- as.factor(filtered_by_subclass_2$subclass)
filtered_by_subclass_2$treatment <- as.factor(filtered_by_subclass_2$treatment)
filtered_by_subclass_2$independence_timing <- as.factor(filtered_by_subclass_2$independence_timing)
filtered_by_subclass_2$field <- as.factor(filtered_by_subclass_2$field)

# this DiD model unfortunately doesnt work for some reason that I cannot resolve, but it is possible to calculate the same using repeated measures anova or mixes effects model, so it should be fine
#---> maybe if I edit the dataset so that there is only one row for each observation and it has columns RII_before and RII_after? ------or alternatively I can also just calculate the difference directly RII_diff = after-before and then test only this difference? Not sure how much of a difference this would make -> I should perhaps ask chatGPT


# # Run regression model with interaction term
# did_model <- plm::plm(RII ~ independence_timing * treatment + factor(subclass),
#                  data = filtered_by_subclass_1,
#                  index = c("subclass"),
#                  model = "within")
# 
# # Calculate clustered standard errors
# clustered_se <- coeftest(did_model, vcov = function(x) vcovHC(x, type = "HC1", cluster = "subclass"))


model2 <- lmer(RII ~ treatment * independence_timing + (1 | subclass), data = filtered_by_subclass_2)
summary(model2)
r.squaredGLMM(model2)


result2 <- ezANOVA(data = filtered_by_subclass_2,
                 dv = RII,
                 wid = subclass,
                 within = .(independence_timing),
                 between = treatment)
print(result)



## adding disciplines:


result_disc2 <- ezANOVA(data = filtered_by_subclass_2,
                 dv = RII,
                 wid = subclass,
                 within = .(independence_timing),
                 between = .(treatment, field))
print(result_disc2)






### visualisations
     
     
means2 <- aggregate(RII ~ treatment + independence_timing, data = filtered_by_subclass_2, FUN = mean)

 ggplot(data = filtered_by_subclass_2, aes(x = treatment, y = RII, fill = independence_timing)) +
         geom_violin(scale = "width") +
         scale_fill_discrete(name = "Independence Timing", labels = c("Before intervention", "After intervention")) +
         xlab("Treatment Group") +
         ylab("RII Score") +
         ggtitle("Violin Plot of RII Scores by Treatment and Time") +
         theme_bw()


 
means_disc2 <- aggregate(RII ~ field, data = filtered_by_subclass_2, FUN = mean)
     
num_disc2 <- filtered_by_subclass_2 %>%
  group_by(field) %>%
  summarise(count = n(), .groups = "drop")

means_disc2 <- left_join(means_disc2, num_disc2, by = "field") 

# Step 1: Reorder the levels of 'field'
means_disc2$field <- with(means_disc2, factor(field, levels=unique(field[order(RII)])))

# Step 2: Optionally, sort the dataframe based on the reordered levels
means_disc2 <- means_disc2 %>% arrange(field)

    
ggplot(data = means_disc2, aes(x = reorder(c(1:6), RII), y = RII, fill = field)) +
  geom_bar(stat = "identity", position = "dodge") +
  
  # Add the number of observations on top of the bars
  geom_text(aes(label=count), position=position_dodge(width=0.9), vjust=-0.5) +

  labs(x = "Field", y = "Mean RII") +
  scale_fill_discrete(name = "Field") +
  theme_bw()

 
```


In the second analysis, we examined the effects of treatment, independence_timing, and their interaction on the researcher independence (RII) for the treatment group and funded control group. Again, there was a significant main effect of time on independence (F(`r result2$ANOVA[2, 2]`, `r result2$ANOVA[2, 3]`) = `r result2$ANOVA[2, 4]`, p = `r result2$ANOVA[2, 5]`), with moderate effect size (GES = `r result2$ANOVA[2, 7]`).
Importantly, this time the interaction effect between treatment and time  was not significant (F(`r result2$ANOVA[3, 2]`, `r result2$ANOVA[3, 3]`) = `r result2$ANOVA[3, 4]`, p = `r result2$ANOVA[3, 5]`), suggesting that funding has the same effects, regardless of its type or source.

When adding disciplines into the analysis, we again found that there is a variation in independence across disciplines (F(`r result_disc2$ANOVA[2, 2]`, `r result_disc2$ANOVA[2, 3]`) = `r result_disc2$ANOVA[2, 4]`, p = `r result_disc2$ANOVA[2, 5]`) with Agricultural and veterinary sciences again showing the lowest independence, while Social Sciences and Humanities showing largest (though it is important to note that small sample might provide unstable estimates (----> i SHOULD REMOVE THEM FROM THE ANALYSIS SINCE THESE SEEM TO BE TOO FEW OBSERVATIONS --> MAYBE I SHOULD ALSO DO A SEPARATE ANOVA ONLY TAKING INTO ACCOUNT ALL DISC BEFORE INTEVENTION AND AFTER INTERVENTION SEPARATELY BUT FOR BOTH CONTROL GROUPS), and with a moderate effect size (GES = `r result_disc2$ANOVA[2, 7]`). Anyway, these differences were again stable across time and treatment conditions as the interaction were not significant.





# Discussion

The results of this study provide important insights into the effects of funding interventions on researcher independence. The strongest effect we found across all control group comparison was the effect of time - over time, researchers become more independent, regardless of their access to funding or their discipline. The effect size for this difference is moderate when comparing funded and unfunded groups, and also moderate when comparing the funded treatment and funded control groups.

Additionally, the observed variation in independence across different disciplines underscores the importance of considering disciplinary differences, suggested by a moderate effect size. A moderate effect size for the field effect emphasizes that certain disciplines might inherently have variations in RII, perhaps due to differences in practices, methodologies, or norms specific to each discipline. Anyway, these differences remain constant across time and funding interventions, suggesting that funding has a uniform effect on the independence across disciplines.

Finally, the significant interaction between treatment and time when comparing treamtnet vs unfunded group suggests the funding has positive effect on stimulating researchers' independence. However, the effect size is only modest, indicating relatively small practical relevance. Further the insignificant interaction when comparing the treatment vs funded control group suggests that funding in this career stage has the same effect, regardless of its type or source.




## Limitations

--> only talks about the Czech environment and considers researchers growing up and most of their careers in the Czech republic

# Attachments

## Identification of supervisors

Data about PhD student-PhD advisor connections are usually not available. One must search for these connections manually on the internet, which is time consuming and not fully reliable (we were only able to find about 69 % of these connections in our treatment group). Therefore, we tried to come up with an alternative way of identifying supervisors from the available data: we assumed that since many researchers only start their publication career during their PhD studies, their PhD supervisors might often coauthor their first publications. That led us to creating an algorithm to identify the supervisor solely from the student's publication history as "the most frequent co-author of the first 5 publications".

To check whether we can use "the most frequent co-author of the first 5 publications" as a proxy to identify supervisors, we have compared the supervisors identified via this method with the manually found supervisors for each researcher in our sample.
In the treatment group of `r nrow(tar_read(ids_complete))`researchers,  118 (49 %) had their most frequent co-author same as their supervisor and 86 (36 %) had someone other than their supervisor as their most frequent co-author in their first 5 publications, and 37 (15%) had no co-author in their first 5 publications.  
In the control groups, we have manually found supervisors for `r nrow(tar_read(sup_complete) %>% filter(!is.na(sup_vedidk)) %>% distinct(name_first, name_last))` researchers, and when we compared them to the supervisors suggested by the most frequent coauthor algorithm, we found that 41 (29 %) had their most frequent co-author the same as their supervisor, 80 (57 %) had someone other than their supervisor as their most frequent co-author in their first 5 publications, and 20 (14 %) had no co-author in their first 5 publications.

Further, we have conducted analysis on an independent sample of researchers from Slovenia (citace Luzar??), and found that  when we filter out all researchers who have not had any publication 4 years prior to their PhD defense (1135, approx 9,5% of the sample), we see that from the remaining researchers, about 50,3 % had their supervisor as the most frequent coauthor in the 4 years prior to their PhD defense.



# References

